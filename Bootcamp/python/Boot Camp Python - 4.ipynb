{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Computing Boot Camp\n",
    "# Boston University "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website: [rcs.bu.edu](http://www.bu.edu/tech/support/research/) <br>\n",
    "Tutorial materials: [https://github.com/bu-rcs/bu-rcs.github.io/tree/main/Bootcamp](https://github.com/bu-rcs/bu-rcs.github.io/tree/main/Bootcamp)\n",
    "\n",
    "# Python Part 4:  Data Analysis\n",
    "\n",
    "In this final tutorial we'll look at some tools used to do analyze data using Python, Pandas, and some additional libraries.\n",
    "\n",
    "Not demo'd here, but as a reference this library is very popular for machine learning style data analysis in Python: scikit-learn (https://scikit-learn.org/stable/) \n",
    "\n",
    "From their website:\n",
    "* Simple and efficient tools for predictive data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Repeating the steps from part 3, let's get the complete data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries.  It's good practice to put all library loading at the top of the notebook.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 100 # for larger plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the US-wide health data.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/bu-rcs/bu-rcs.github.io/main/Bootcamp/Data/USA_HealthData.csv')\n",
    "# Clean out the NaN County rows, as before\n",
    "df.drop(df[df['County'].isnull()].index, inplace=True) \n",
    "# And delete the FIPS column for plotting convenience\n",
    "del df['FIPS']\n",
    "# Bring in the census regions\n",
    "reg_df = pd.read_csv('https://raw.githubusercontent.com/bu-rcs/bu-rcs.github.io/main/Bootcamp/Data/us_states_census_regions.csv')\n",
    "df = pd.merge(left=df, right=reg_df, left_on='State', right_on='State')\n",
    "\n",
    "# And the US-wide demographic data\n",
    "demo_df = pd.read_csv('https://raw.githubusercontent.com/bu-rcs/bu-rcs.github.io/main/Bootcamp/Data/USA_DemographicsData.csv')\n",
    "demo_df.drop(demo_df[demo_df['County'].isnull()].index, inplace=True) \n",
    "del demo_df['FIPS']\n",
    "\n",
    "# Merge in the demographic data by county and state\n",
    "df = pd.merge(left=df, right=demo_df, left_on=['County','State'], right_on=['County','State'])\n",
    "# Let's rename a few columns for convenience\n",
    "rename={\"Life Expectancy\":'life_exp', \"% Frequent Physical Distress\":'ph_distress',\n",
    "        \"% Limited Access to Healthy Foods\":'limit_healthy_food',\"# Black\":'n_black',\n",
    "       \"# Rural\":'n_rural',\"Median Household Income\":'house_income',\n",
    "       'Population':'pop'}\n",
    "df = df.rename(columns=rename)\n",
    "\n",
    "# Preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law\n",
    "\n",
    "https://en.wikipedia.org/wiki/Zipf's_law\n",
    "\n",
    "First, let's do some curve fitting (aka regression) to the data. Zipf's Law is a empirical law first observed in language: \n",
    ">...given a large sample of words used, the frequency of any word is inversely proportional to its rank in the frequency table. So word number n has a frequency proportional to 1/n.\n",
    "\n",
    "As has been observed many times, this is true for things like the [population of cities](https://arxiv.org/ftp/arxiv/papers/1402/1402.2965.pdf) and the frequency of the size of the cities .  Let's see if we can observe Zipf's law this when looking at US county populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll need the population of each county.  No need to work with the whole dataframe, let's just use \n",
    "# a Series. We want to then sort and modify this Series in-place (i.e. without making new Series or Dataframes)\n",
    "# so force a copy to be made.\n",
    "\n",
    "# Look at populations >10,000\n",
    "pop = df[df['pop'] > 10000][['pop']].copy()\n",
    "\n",
    "# Next, sort in descending order.\n",
    "pop.sort_values(ascending=False, inplace=True,by='pop')\n",
    "pop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need an index starting at 1 to the size of the Series\n",
    "pop['ind'] = np.arange(1,pop.size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot it!\n",
    "ax=pop.plot(x='ind',y='pop')\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_ylabel('Population');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot it on a log-log scale\n",
    "ax=pop.plot(x='ind',y='pop',loglog=True)\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_ylabel('Population');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And how about just the ones where the rank is <= 100?\n",
    "# This is for countries > ~675,000 people\n",
    "pop_sub = pop[pop['ind'] < 100]\n",
    "ax=pop_sub.plot(x='ind',y='pop',loglog=True)\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_ylabel('Population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law as originally formulated has the form:   p = r<sup>-1</sup>\n",
    "\n",
    "On the last log-log plot for counties >= ~675,000 we see a straight line, suggesting a power law relationship.\n",
    "\n",
    "Let's do a fit of the form p = B r<sup>A</sup> and see what we get for the fitting parameters A and B.\n",
    "\n",
    "Two ways:\n",
    "* Linear least squares after taking the log (base 10) of the equation:  log10(p) = A log10(r) + B<sub>0</sub>\n",
    "* Non-linear least squares to p = r<sup>A</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear first.  Use a convenient function from scipy:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "result = sp.stats.linregress(np.log10(pop_sub['ind']), np.log10(pop_sub['pop']))\n",
    "# What's in the result?  Check the docs\n",
    "lin_A = result.slope\n",
    "lin_B = 10**result.intercept\n",
    "print('A = %1.4f  B = %3.4f'  % (lin_A, lin_B))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear without taking the log of the equation. This uses another scipy function:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html\n",
    "# This requires a Python function that computes residuals, i.e. the difference between\n",
    "# the result of the equation with the guessed parameter A and the y values.\n",
    "def residuals(coeffs, x, y):\n",
    "    # coeffs in an array of size 1 \n",
    "    # coeffs[0] --> A\n",
    "    # coeffs[1] --> B\n",
    "    return y - coeffs[1] * x**coeffs[0]\n",
    "\n",
    "# An initial guess is needed for A. Let's try -1.\n",
    "# To get this to work we'll need to force it to only consider values for A less than 1.\n",
    "result = sp.optimize.least_squares(residuals, [-1.0,1.0], args=(pop_sub['ind'], pop_sub['pop']))\n",
    "nonlin_A = result.x[0]\n",
    "nonlin_B = result.x[1]\n",
    "print('A = %1.4f  B = %3.4f'  % (nonlin_A, nonlin_B))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results!\n",
    "x0 = pop_sub['ind']\n",
    "y0 = pop_sub['pop']\n",
    "y1 = lin_B * x0**lin_A\n",
    "y2 = nonlin_B * x0**nonlin_A\n",
    "plt.loglog(x0,y0,'.')\n",
    "plt.loglog(x0,y1,'-')\n",
    "plt.loglog(x0,y2,'-')\n",
    "plt.legend(['data','linear lsq','non-lin lsq'])\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('County Population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now...I wonder if this holds for States too...\n",
    "state_pop = df.groupby('State').sum()['pop'].copy()\n",
    "state_pop.sort_values(ascending = False, inplace=True)\n",
    "state_ind = np.arange(1,52) # numbers 1 to 51...including Washington, D.C.\n",
    "# Fit to the 30 biggest states\n",
    "state_res = sp.stats.linregress(np.log10(state_ind[0:30]), np.log10(state_pop.iloc[0:30]))\n",
    "# What's in the result?  Check the docs\n",
    "lin_A = state_res.slope\n",
    "lin_B = 10**state_res.intercept\n",
    "plt.figure()\n",
    "plt.loglog(state_ind, state_pop,'.')\n",
    "plt.loglog(state_ind, lin_B * state_ind**lin_A)\n",
    "plt.legend(['data','linear lsq'])\n",
    "print('A = %3.4f   B=%3.4f' % (lin_A, lin_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "* State Hypothesis\n",
    "* Formulate an analysis plan\n",
    "* Perform analysis\n",
    "* Interpret results\n",
    "\n",
    "## Checking if a variable is normally distributed\n",
    "Let's look at life expectancy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Uncomment this line and comment the sns line to compare plotting styles\n",
    "#df.hist('Life_Expectancy',ax=ax, bins=np.arange(60, 100, 0.5))\n",
    "ax.hist(df['life_exp'], bins=np.arange(60, 100, 0.5))\n",
    "med_age = df['life_exp'].median()\n",
    "# How high to plot the median line?  Let's query the axes for\n",
    "# the ylimits:\n",
    "ylim = ax.get_ylim() # ylim == (0.0, 1167.6)\n",
    "ax.plot([med_age,med_age],[0,ylim[1]],'r--')\n",
    "ax.set_xlabel('age')\n",
    "ax.set_ylabel('Count of counties')\n",
    "ax.set_title('Life Expectancy')\n",
    "# Set the y axis to the range it was at before we added the median line\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim([60,100])\n",
    "ax.legend(['Median age: %2.1f' % med_age])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this normally distributed?  Let's overlay a Gaussian curve...\n",
    "# Get the histogram values without any nan values.   \n",
    "clean_life_exp = df[df['life_exp'] > 0]['life_exp'] \n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "# The curve_fit function is a general non-linear fitting tool.  It takes\n",
    "# a function as before with least_squares above.\n",
    "hist, bin_edges = np.histogram(clean_life_exp, density=True, bins=np.arange(60,100,0.5))\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:])/2\n",
    "\n",
    "# Define model function to be used to fit to the data above:\n",
    "def gauss(x, A, mu, std):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*std**2))\n",
    "\n",
    "# Pick some guesses for A, mu, std\n",
    "p0 = [1., np.mean(clean_life_exp), 1]\n",
    "coeff, var_matrix = curve_fit(gauss, bin_centers, hist, p0=p0)\n",
    "# Extract the coefficients\n",
    "A, mu,std=coeff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot it\n",
    "fig, ax = plt.subplots()\n",
    "# density=True will normalize the plot of the histogram\n",
    "ax.hist(df['life_exp'],  density=True, bins=np.arange(60, 100, 0.5))\n",
    "med_age = df['life_exp'].median()\n",
    "# How high to plot the median line?  Let's query the axes for\n",
    "# the ylimits:\n",
    "ylim = ax.get_ylim() # ylim == (0.0, 1167.6)\n",
    "ax.plot([med_age,med_age],[0,ylim[1]],'r--')\n",
    "ax.set_xlabel('age')\n",
    "ax.set_ylabel('Count of counties')\n",
    "ax.set_title('Life Expectancy')\n",
    "# Set the y axis to the range it was at before we added the median line\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim([60,100])\n",
    "ax.legend(['Median age: %2.1f' % med_age])\n",
    "\n",
    "# Use the norm sub-sub-library from scipy to compute the PDF for the plot.\n",
    "from scipy.stats import norm\n",
    "hist, bin_edges = np.histogram(clean_life_exp, density=True, bins=np.arange(60, 100, 0.5))\n",
    "p = norm.pdf(bin_edges, mu, std)\n",
    "ax.plot(bin_edges, p, 'k', linewidth=2)\n",
    "# Let's check whether this fits well more rigourously..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Hypothesis:\n",
    "* Null-hypothesis - the sample comes from a normal distribution. \n",
    "* Alternative hypothesis - the sample does not come from a normal distribution\n",
    "\n",
    "### Formulate an analysis plan\n",
    "\n",
    "We will run [Shapiro-Wilks](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) test of normality.\n",
    "\n",
    "### Perform analysis\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n",
    "\n",
    "Scipy has a Shapiro-Wilks test included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.shapiro(df['life_exp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we still need to remove the NaN values!\n",
    "clean_life_exp = df[df['life_exp'] > 0]['life_exp']\n",
    "result = sp.stats.shapiro(clean_life_exp)\n",
    "# What's in the result?  try: dir(result)\n",
    "print('p-value: %3.5e' % result.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despite all the very high-level functions we're using don't forget we're still \n",
    "# writing regular Python code!  All the regular Python language features are available\n",
    "# all the time, of course.  Let's have Python figure out the answer...\n",
    "if result.pvalue  < 0.05:\n",
    "    print('Null hypothesis is rejected, not from a normal distribution.')\n",
    "else:\n",
    "    print('Null hypothesis is accepted, samples are from a normal distribution.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two samples\n",
    "\n",
    "Let’s compare life expectancy in New England and East South Ctr Region. Let’s first summarize our data and find median and mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group, extract just the life_exp column to avoid doing more calculations than necessary,\n",
    "# do the calculations.\n",
    "df.groupby('Region')[['life_exp']].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s visually compare two distributions of this variable for both regions\n",
    "# Make a view onto just the two regions of interest\n",
    "reg_df = df[df['Region'].isin(['East South Ctr','New. Eng.'])][['Region','life_exp']]\n",
    "reg_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to put two histograms onto on axes...\n",
    "bins = np.arange(60,100,0.5)\n",
    "# Two sets of data\n",
    "esc = reg_df[reg_df['Region']=='East South Ctr']['life_exp']\n",
    "ne = reg_df[reg_df['Region'] == 'New. Eng.']['life_exp']\n",
    "\n",
    "# Let's check to see if distributions for each region\n",
    "# are normally distributed...\n",
    "\n",
    "# Put repetitive things into functions.\n",
    "def print_is_norm(var, name):\n",
    "    result = sp.stats.shapiro(ne)\n",
    "    if result.pvalue  < 0.05:\n",
    "        print('%s is not from a normal distribution.' % name)\n",
    "    else:\n",
    "        print('%s samples is from a normal distribution.' % name)\n",
    "    \n",
    "print_is_norm(esc, 'East South Ctr')\n",
    "print_is_norm(ne, 'New. Eng.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note a new way to do a vertical line...\n",
    "plt.hist(esc, bins, alpha=0.5, label='East South Ctr')\n",
    "# **** Why was this variable created?\n",
    "esc_avg = esc.mean()\n",
    "plt.axvline(esc_avg, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.hist(ne, bins, alpha=0.5, label='New. Eng.')\n",
    "ne_avg = ne.mean()\n",
    "plt.axvline(ne_avg, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "# Add labels to the median lines\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "# Positioning this text is a little laborious. text() takes 3 args:\n",
    "# x-position / y position to start the text, and a string\n",
    "plt.text(esc_avg*0.86, max_ylim*0.9, 'Mean: %1.2f' % (esc_avg,))\n",
    "plt.text(ne_avg*1.005, max_ylim*0.5, 'Mean: %1.2f' % (ne_avg,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Hypothesis:\n",
    "* Null-hypothesis -  difference in means is equal to 0. \n",
    "* Alternative hypothesis - true difference in means is not equal to 0.\n",
    "\n",
    "In other words, is the difference in mean value between these two regions statistically significant?\n",
    "\n",
    "### Formulate an analysis plan\n",
    "\n",
    "We will run [Welch's Two Sample t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test).  This applies when two sets of samples are both from normal distributions.\n",
    "\n",
    "\n",
    "### Perform analysis\n",
    "Scipy routine: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.ttest_ind(esc, ne, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm...nan values are still causing trouble.  This particular\n",
    "# function has an option to handle them\n",
    "sp.stats.ttest_ind(esc, ne, equal_var=False, nan_policy='omit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Interpret results\n",
    "\n",
    "From the output, the p-value < 0.05 implying that the true difference in means is not equal to zero and we can reject the null hypothesis. We then conclude that the difference in mean values in the two regions is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise for Later\n",
    "\n",
    "* Check if the variable house_income normally distributed\n",
    "* Perform Welch t.test to compare house_income between New England Region and East South Ctr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Covariance\n",
    "\n",
    "Let's go back and consider the entire dataset. Pandas can compute a correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default is the Pearson method\n",
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about in visual form?\n",
    "plt.matshow(corr)\n",
    "plt.colorbar()\n",
    "# Our dataset mixes health info with demographics so this is slightly funky looking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn version\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn version again.  This time, overwrite the diagonal \n",
    "# 1's in the correlation matrix with NaN values so that they don't \n",
    "# get displayed.\n",
    "\n",
    "# corr.values will access the underlying Numpy storage for the \n",
    "# dataframe.  As this is a dataframe entirely of float64 types it's\n",
    "# stored as a single 2D numpy array:\n",
    "print(type(corr.values))\n",
    "print(corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a numpy function to overwrite the diagonals\n",
    "#  https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html\n",
    "# This modification is in-place.  Functions like this are likely implemented\n",
    "# in compiled C code and are much faster than doing this using Python for loops.\n",
    "np.fill_diagonal(corr.values, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot\n",
    "sns.heatmap(corr)\n",
    "plt.matshow(corr)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The % Fair Health vs % Smokers has a high correlation - 0.728878\n",
    "# Just to show it off, plot with seaborn with a straight line fit\n",
    "# https://seaborn.pydata.org/generated/seaborn.regplot.html\n",
    "sns.regplot(x='% Smokers',y='% Fair or Poor Health',data=df, fit_reg=True, line_kws={\"color\": \"red\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between these columns\n",
    "corr = df['% Smokers'].corr(df['% Fair or Poor Health'])\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the covariance\n",
    "covar = df['% Smokers'].cov(df['% Fair or Poor Health'])\n",
    "covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Pearson correlation coefficient and p-value \n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
    "\n",
    "r, p_value = sp.stats.pearsonr(df['% Smokers'],df['% Fair or Poor Health'])\n",
    "print(\"Pearson's correlation coefficient: %s    P-value: %s\" % (r,p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Table\n",
    "\n",
    "We'll add a column to our dataframe called \"wealth\" which categorizes household income. Then let's make a new dataframe using a spreadsheet-style pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize household income: \n",
    "#    < 50,000  : low\n",
    "#    < 80,000  : high\n",
    "#    in between: middle\n",
    "\n",
    "# Loop over the dataframe rows, build a list of strings, add it\n",
    "# as a new column\n",
    "# Let's look at some different ways to do this...\n",
    "\n",
    "def method_iterrows(df):\n",
    "    wealth = []  # Empty Python list\n",
    "    for row in df.iterrows():\n",
    "        # This returns each row as:\n",
    "        # (row_number, Series)\n",
    "        house_inc = row[1]['house_income']\n",
    "        cat = 'middle'\n",
    "        if house_inc < 50000:\n",
    "            cat = 'low' \n",
    "        elif house_inc > 80000:\n",
    "            cat = 'high' \n",
    "        wealth.append(cat)\n",
    "    return wealth\n",
    "\n",
    "def method_itertuples(df):\n",
    "    wealth = []  # Empty Python list\n",
    "    for row in df.itertuples():\n",
    "        # This returns each row as a \"namedtuple\"\n",
    "        # https://pymotw.com/2/collections/namedtuple.html\n",
    "        house_inc = row.house_income\n",
    "        cat = 'middle'\n",
    "        if house_inc < 50000:\n",
    "            cat = 'low' \n",
    "        elif house_inc > 80000:\n",
    "            cat = 'high' \n",
    "        wealth.append(cat)\n",
    "    return wealth\n",
    "\n",
    "def method_col_loop(df):\n",
    "    wealth = []  # Empty Python list\n",
    "    # Loop over the elements of a column\n",
    "    for house_inc in df['house_income']:\n",
    "        # house_inc is just each value in this column\n",
    "        cat = 'middle'\n",
    "        if house_inc < 50000:\n",
    "            cat = 'low' \n",
    "        elif house_inc > 80000:\n",
    "            cat = 'high' \n",
    "        wealth.append(cat)\n",
    "    return wealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time is a special iPython command that gives an elapsed time for a function call\n",
    "# You can also try %timeit which runs the command multiple times and gives an average elapsed time\n",
    "%time method_iterrows(df)\n",
    "%time method_itertuples(df)\n",
    "%time method_col_loop(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new column\n",
    "df['wealth'] = method_col_loop(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region and count the values of wealth in each region.\n",
    "# The function here is pivot_table\n",
    "# hmmm...couldn't get this to work without a new column \n",
    "df['wealth_'] = df['wealth']\n",
    "wealth_df = df.pivot_table(index='Region',values='wealth',columns='wealth_',aggfunc='count')\n",
    "wealth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
